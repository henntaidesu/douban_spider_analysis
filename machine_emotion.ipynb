{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a7d0518c2278a9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:18:02.470031Z",
     "start_time": "2025-01-17T12:17:58.771091Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.module.execution_db import DB\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "import sys\n",
    "import jieba\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "\n",
    "# pip install scikit-learn tensorflow-gpu==2.10.1 pandas jieba\n",
    "# tensorflow更高版本不支持windows下GPU训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7213d4",
   "metadata": {},
   "source": [
    "# 调用模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2000, 16)          160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,357\n",
      "Trainable params: 160,357\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "预测结果: 正面\n",
      "预测结果: [1.7012000e-03 4.5050649e-16 2.4162831e-11 9.9679273e-01 1.5060488e-03]\n",
      "预测概率: 0.996792733669281\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('sentiment_model.h5')\n",
    "model.summary()  # 查看模型结构\n",
    "\n",
    "VOCAB_SIZE = 10000  # 词典大小上限\n",
    "MAX_LENGTH = 2000  # 修改为合理长度\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<OOV>')\n",
    "\n",
    "# 模拟训练数据初始化 tokenizer（实际应加载训练好的 tokenizer）\n",
    "sample_texts = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "tokenizer.fit_on_texts(sample_texts)\n",
    "\n",
    "text = '''最近央视一套和上海新闻综合频道都在热播《诺尔曼·白求恩》，当初决定看这部电视，一方面是是因为演白求恩的外籍演员还挺帅，一方面是因为看到介绍说将会展现他来到中国前的鲜为人知的前半生，勾起了一点我的好奇心。'''\n",
    "\n",
    "# 1. 清洗和分词\n",
    "text = re.sub(r'[^\\w\\s]', '', text).replace('\\n', '').replace('  ', ' ')\n",
    "text = \" \".join(jieba.cut(text))\n",
    "\n",
    "# 2. 转成序列并 pad\n",
    "seq = tokenizer.texts_to_sequences([text])\n",
    "if not seq or len(seq[0]) == 0:\n",
    "    raise ValueError(\"输入文本未生成有效的序列，请检查分词和 tokenizer 初始化\")\n",
    "padded = pad_sequences(seq, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# 3. 预测\n",
    "pred = model.predict(padded)\n",
    "label_idx = np.argmax(pred, axis=1)[0]\n",
    "if label_idx < 2:\n",
    "    print(\"预测结果: 负面情感\")\n",
    "if label_idx == 2:\n",
    "    print(\"预测结果: 中性情感\")\n",
    "if label_idx > 2:    \n",
    "    print(\"预测结果: 正面情感\")\n",
    "\n",
    "print(f\"预测结果: {pred[0]}\")\n",
    "print(f\"预测概率: {pred[0][label_idx]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b64a61",
   "metadata": {},
   "source": [
    "# 清洗数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b87c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql = f'SELECT `comment_ID`, `comment_text` FROM `douban`.`douban_long_comment` WHERE clean_comment_text is null'\n",
    "# flag, data = DB().select(sql)\n",
    "# for i in data:\n",
    "#     text = re.sub(r'[^\\w\\s]', '', i[1]).replace('\\n', '').replace('  ', '')\n",
    "#     print(text)\n",
    "#     words = jieba.cut(text)\n",
    "#     text = \" \".join(words)\n",
    "#     sql = f\"UPDATE `douban`.`douban_long_comment` SET  `clean_comment_text` = '{text}' WHERE `comment_ID` = {i[0]} \"\n",
    "#     DB().update(sql)\n",
    "#     print(\"-\"* 100)\n",
    "# 列出所有物理设备\n",
    "\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Eager execution enabled:\", tf.executing_eagerly())\n",
    "print(\"Version:\", tf.__version__)\n",
    "print(tf.test.gpu_device_name())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5841e545",
   "metadata": {},
   "source": [
    "# 创建训练数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c14e64fce673bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['rank', 'text'])\n",
    "sql = f'SELECT `star`, `clean_comment_text` FROM `douban`.`douban_long_comment`  WHERE star is not null  limit 3000'\n",
    "flag, data = DB().select(sql)\n",
    "\n",
    "for i in data:\n",
    "    if i[0] == '力荐':\n",
    "        rank = 5\n",
    "    elif i[0] == '推荐':\n",
    "        rank = 4\n",
    "    elif i[0] == '还行':\n",
    "        rank = 3\n",
    "    elif i[0] == '较差':\n",
    "        rank = 2\n",
    "    else:\n",
    "        rank = 1\n",
    "\n",
    "    text = i[1]\n",
    "\n",
    "    row = pd.DataFrame([{'rank': rank, 'text': text}])\n",
    "    df = pd.concat([df, row], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# 将数据划分为训练集和测试集\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['text'].values,\n",
    "    df['rank'].values,\n",
    "    test_size=0.2,  # 测试集占 20%\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 构建词典，并将文本转化为序列\n",
    "VOCAB_SIZE = 10000  # 词典大小上限\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# 设定句子最大长度，过长的句子会被截断，过短的会用 0 填充\n",
    "MAX_LENGTH = 2000\n",
    "train_padded = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "# 处理标签（1~5）为多分类所需的 0~4 (再 one-hot)\n",
    "num_classes = 5\n",
    "train_labels = train_labels - 1  # [1,5] -> [0,4]\n",
    "test_labels = test_labels - 1  # 同上\n",
    "\n",
    "train_labels_onehot = tf.keras.utils.to_categorical(train_labels, num_classes=num_classes)\n",
    "test_labels_onehot = tf.keras.utils.to_categorical(test_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c3a9a6",
   "metadata": {},
   "source": [
    "# 构建与训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505e219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=VOCAB_SIZE,\n",
    "                    output_dim=embedding_dim,\n",
    "                    input_length=MAX_LENGTH))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 训练模型\n",
    "EPOCHS = 500 # 训练轮数\n",
    "history = model.fit(\n",
    "    train_padded,\n",
    "    train_labels_onehot,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(test_padded, test_labels_onehot),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "model.save('sentiment_model.h5')\n",
    "\n",
    "sys.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "douban_machine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
